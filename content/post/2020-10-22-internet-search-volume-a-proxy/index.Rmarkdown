---
title: 'Internet Search Volume: A Proxy'
author: ''
date: '2020-10-22'
slug: []
categories: []
tags: []
---


## Introduction 
Internet search volume and web page hits can provide interesting insights into the collective unconscious (read the minds of bored 20 - somethings on furlough) 


This is nice quick introduction to two straight forward packages, `googletrendsR` and `wikipediatrend`. 

There will also be a cursory flirt with Facebook's `Prophet`, a forecasting package.

So lets call in our libraries 
```{r,warning=FALSE,message=FALSE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
options(scipen = 999)
library(gtrendsR)
library(tidyverse)
library(ggcharts)
library(ggsci)
library(ggthemes)
```


## Google Trends

The query requires a few different arguments which we can define before hand.
The `+` between the terms in the keyword vector ensures that those words appear in that order. 

```{r}
#define the keywords
keywords <- c('Artificial+neural+network')
# Choose Location  --------------------------------------------------------
country=c('GB')
# Define Time Fram -------------------------------------------------
time_frame = ("2014-01-01 2020-05-30")
#set channels 
channel='web'
```

Here we have the final query, which will return us a `list` of `dataframes`
```{r}
trends  <-  gtrends(keywords, gprop = channel, geo = 'GB', time = time_frame )
str(trends)
```



We can access the elements of the list we would like and assign them to a variable. I have creatively named them as seen below. 

The city data seems quite suspect to me and is undoubtedly a case of search locations being grouped to the nearest
'city'
```{r}

time_trend <- trends$interest_over_time

city <- trends$interest_by_city

```


### Process and plot

```{r}
# A little bit of cleaning ------------------------------------------------
library(forcats)
city <- na.omit(city)
# Lets take a look --------------------------------------------------------
ggplot(city, aes( reorder(location, hits), hits)) +
geom_col(fill ="#E69F00", show.legend = FALSE) +
coord_flip() +
labs(title = 'Search Results By Approx Location',
        y = 'Relative Interest',
        x ='Approx Locations',
       caption = Sys.Date()) +
theme_hermit()

```



```{r}

# CAUTION: This step is only undertaken to ensure better plotting  --------
time_trend$hits[time_trend$hits == '<1'] <- 0

time_trend$hits <- as.numeric(time_trend$hits)

glimpse(time_trend)

ggplot(time_trend, aes(x=date, y=hits,group=keyword,col=keyword))+
  geom_line(size=1,col ="#E69F00")+
  xlab('Time')+
  ylab('Relative Interest')+ 
  theme_hermit()+
  theme(legend.title = element_blank(),legend.position="right",legend.text=element_text(size=12))+
  ggtitle('Interest in Artificial Neural Networks since 2014') +
  scale_color_viridis_d()


```



# Lets do something similar this time in Wikipedia
Using Wikipedia Trend


The first thing we will do is create the query. Note that you will have to ensure
that the wikipedia you want exists and that you have written it as it appears in
actual URL.

```{r}
library(wikipediatrend)

wiki_search <- wp_trend(page = 'Artificial_neural_network',
         from = '2014-01-01',
         to = Sys.Date())

```






```{r}
wiki_search %>% 
  ggplot(aes(date,views,col=article),show.legend=FALSE) +
  geom_line(size=0.5,show.legend = FALSE)+
  geom_smooth(aes(col="#E69F00",se = FALSE,),show.legend=FALSE)+
  theme_hermit() +
  scale_y_continuous(labels = scales::comma)+
  scale_color_viridis_d() +
  labs(title = 'Wikipedia Page Views Since Jan 2014')
```



# Lets try some forecasting 
## Be it a very quick and dirty forecast



Facebook's Prophet requires two columns with specific names `ds` and `y`
```{r}
library(prophet)
ds <- time_trend$date
y <- time_trend$hits

```

you can then combine both into a `dataframe`


```{r}
df <- data.frame(ds,y)
```

Make a prophet object
```{r}
m <- prophet(df,weekly.seasonality = TRUE)

class(m)
```

How far you want to forecast, `periods` is in days
```{r}
future <- make_future_dataframe(m, periods = 365)
```
 Use the native predict function
```{r}
forecast <- predict(m, future)
```
Use `Base R` for some plotting 
```{r}
plot(m, forecast)
```
View the decomp
```{r}
prophet_plot_components(m, forecast)
```

This has been quite a quick and dirty  forecast and should without a doubt
**NOT** be considered a true reflection of patterns. 


